{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'cfg_decoding.logits_processor' from '/workspaces/funcqa_experiments/cfg_decoding/logits_processor.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import LogitsProcessor, AutoModelForCausalLM, AutoTokenizer, BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
    "\n",
    "import cfg_decoding.parsing as p\n",
    "import cfg_decoding.logits_processor as lp\n",
    "\n",
    "import importlib\n",
    "importlib.reload(p)\n",
    "importlib.reload(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-13b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State(start_idx=0, terminals={'__ANON_0'})\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(p)\n",
    "\n",
    "with open(\"funcqa.lark\", \"r\") as f:\n",
    "    cfg_def = f.read()\n",
    "\n",
    "stepper = p.create_parsing_stepper(cfg_def, tokenizer)\n",
    "\n",
    "print(stepper.get_parsing_state(\"add(1\"))\n",
    "\n",
    "# s = 'add(10., 2.)'\n",
    "# for i in range(len(s)+1):\n",
    "#     cfg_state = stepper.get_parsing_state(s[:i])\n",
    "#     print(f\"'{s[:i]}' -> {cfg_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:54<00:00, 38.27s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_4bit=True, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use functions add, mul, div and sub to solve the following math problem.\n",
      "\n",
      "E.g. multiply(1, 20) or add(1, mul(2, 3)) or divide(5, 3) or subtract(15, 3) or add(10, 2)\n",
      "\n",
      "Question: 1 + 20\n",
      "\n",
      "Calculation: <T>add(1,20)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(lp)\n",
    "\n",
    "num_beams = 10\n",
    "input_prompt = '''Use functions add, mul, div and sub to solve the following math problem.\n",
    "\n",
    "E.g. multiply(1, 20) or add(1, mul(2, 3)) or divide(5, 3) or subtract(15, 3) or add(10, 2)\n",
    "\n",
    "Question: 1 + 20\n",
    "\n",
    "Calculation: '''\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    input_prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "input_ids = torch.stack([input_ids] * num_beams, dim=0).reshape(num_beams, -1).to(model.device)\n",
    "bos_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long) * model.config.bos_token_id\n",
    "input_ids = torch.cat([bos_ids, input_ids], dim=-1)\n",
    "\n",
    "prompt_end_index = input_ids.shape[1]\n",
    "max_length = prompt_end_index + 20\n",
    "\n",
    "final_sentence = model.beam_search(\n",
    "    input_ids, \n",
    "    beam_scorer=BeamSearchScorer(\n",
    "        batch_size=1,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        device=\"cuda\",\n",
    "        length_penalty=1.0,\n",
    "        do_early_stopping=True,\n",
    "    ),\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        lp.GrammarConstrainedLogitsProcessor(tokenizer, stepper, prompt_end_index=prompt_end_index)\n",
    "    ]),\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxLengthCriteria(max_length=max_length)\n",
    "    ]),\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")\n",
    "\n",
    "final_sentence_str = tokenizer.batch_decode(final_sentence, skip_special_tokens=True)[0]\n",
    "print(final_sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = model.generate(\n",
    "    input_ids[0:1], \n",
    "    logits_processor = LogitsProcessorList([\n",
    "        lp.GrammarConstrainedLogitsProcessor(tokenizer, stepper, prompt_end_index=prompt_end_index)\n",
    "    ]),\n",
    "    max_new_tokens=40,\n",
    "    # stopping_criteria = StoppingCriteriaList([\n",
    "    #     MaxLengthCriteria(max_length=max_length)\n",
    "    # ]),\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,     1,  4803,  3168,   788, 29892, 15065, 29892,  1933,   322,\n",
       "         1014,   304,  4505,   278,  1494,  5844,  1108, 29889,    13,    13,\n",
       "        29923, 29889, 29887, 29889, 22932, 29898, 29896, 29892, 29871, 29906,\n",
       "        29900, 29897,   470,   788, 29898, 29896, 29892, 15065, 29898, 29906,\n",
       "        29892, 29871, 29941,   876,   470, 16429, 29898, 29945, 29892, 29871,\n",
       "        29941, 29897,   470, 23197, 29898, 29896, 29945, 29892, 29871, 29941,\n",
       "        29897,   470,   788, 29898, 29896, 29900, 29892, 29871, 29906, 29897,\n",
       "           13,    13, 16492, 29901, 29871, 29896,   718, 29871, 29906, 29900,\n",
       "           13,    13, 27065,   362, 29901, 29871, 29966, 29911, 29958,  1202,\n",
       "        29898, 29896, 29892, 29906, 29900, 29897,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<T>add(1,20)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(text[:, prompt_end_index:], skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
