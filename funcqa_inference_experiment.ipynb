{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "from lark import UnexpectedInput, Lark, UnexpectedCharacters, UnexpectedToken, UnexpectedEOF, UnexpectedInput\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import numpy as np\n",
    "from transformers import LogitsProcessor, AutoModelForCausalLM, AutoTokenizer, BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(1, {'RSQB', 'COMMA'})\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "with open(\"cfg_json.lark\", \"r\") as f:\n",
    "    cfg_json = f.read()\n",
    "\n",
    "json_parser = Lark(\n",
    "    cfg_json, \n",
    "    parser='lalr',\n",
    "    # Using the basic lexer isn't required, and isn't usually recommended.\n",
    "    # But, it's good enough for JSON, and it's slightly faster.\n",
    "    lexer='basic',\n",
    "    # Disabling propagate_positions and placeholders slightly improves speed\n",
    "    propagate_positions=False,\n",
    "    maybe_placeholders=False,\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\") \n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "state = ParsingStepper(json_parser, vocab, tokenizer.eos_token)\n",
    "str(state.get_parsing_state('[null'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' -> (0, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{' -> (0, {'RBRACE', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"' -> (1, {'RBRACE', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"a' -> (1, {'RBRACE', 'ESCAPED_STRING'})\n",
      "'{\"a\"' -> (1, {'COLON'})\n",
      "'{\"a\":' -> (4, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": ' -> (4, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": [' -> (6, {'RSQB', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"a\": [\"' -> (7, {'RSQB', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL', 'ESCAPED_STRING'})\n",
      "Second catch\n",
      "'{\"a\": [\"1' -> (7, {'RSQB', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL', 'ESCAPED_STRING'})\n",
      "'{\"a\": [\"1\"' -> (7, {'RSQB', 'COMMA'})\n",
      "'{\"a\": [\"1\",' -> (10, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": [\"1\", ' -> (10, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"' -> (12, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b' -> (12, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})\n",
      "'{\"a\": [\"1\", \"b\"' -> (12, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\":' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": ' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\",' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", ' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\",' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", ' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]]' -> (15, {'RSQB', 'COMMA'})\n",
      "Second catch\n",
      "'{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]]}' -> (15, {'RSQB', 'COMMA'})\n"
     ]
    }
   ],
   "source": [
    "s = '{\"a\": [\"1\", \"b\": [\"1\", \"2\", \"3\"]]}'\n",
    "for i in range(len(s)+1):\n",
    "    cfg_state = state.get_parsing_state(s[:i])\n",
    "    print(f\"'{s[:i]}' -> {cfg_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, {'ESCAPED_STRING'})\n",
      "{\"num_values\": _\"4\", \"values\": [\"1\", \"2\",\">\",\">\"],\n"
     ]
    }
   ],
   "source": [
    "s = '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">\",\">\"],'\n",
    "print(state.get_parsing_state(s))\n",
    "print(s[:15] + \"_\" + s[15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-01 15:28:11.464215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-01 15:28:13.605689: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:58<00:00, 29.22s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",']\n",
      "Parsing states: [(39, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (39, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([4304, 4304], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">']\n",
      "Second catch\n",
      "Parsing states: [(40, {'RSQB', 'COMMA'}), (40, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [[',', ']'], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([29892,   613], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">\",']\n",
      "Parsing states: [(44, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (43, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([4304, 1013], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",\">\",\">']\n",
      "Second catch\n",
      "Parsing states: [(45, {'RSQB', 'COMMA'}), (44, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'})]\n",
      "Valid tokens: [[',', ']'], ['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '[']]\n",
      "Argmax: tensor([29892,  3108], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null]']\n",
      "Parsing states: [(49, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (49, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,{']\n",
      "Parsing states: [(50, {'RSQB', 'COMMA'}), (50, {'RBRACE', 'ESCAPED_STRING'})]\n",
      "Valid tokens: [[',', ']'], ['\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', '\"]', '\"><', '\",\"', '\"/>', '\":\"', '\"),', '\"></', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', '\"', '}']]\n",
      "Argmax: tensor([29892, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null]']\n",
      "Parsing states: [(54, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (54, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,false']\n",
      "Parsing states: [(55, {'RSQB', 'COMMA'}), (55, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null]']\n",
      "Parsing states: [(59, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (59, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,false']\n",
      "Parsing states: [(60, {'RSQB', 'COMMA'}), (60, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null]']\n",
      "Parsing states: [(64, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (64, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,false']\n",
      "Parsing states: [(65, {'RSQB', 'COMMA'}), (65, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null]']\n",
      "Parsing states: [(69, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (69, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,false']\n",
      "Parsing states: [(70, {'RSQB', 'COMMA'}), (70, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null]']\n",
      "Parsing states: [(74, {'ESCAPED_STRING', 'TRUE', 'LSQB', 'FALSE', 'LBRACE', 'NULL'}), (74, {'RBRACE', 'COMMA'})]\n",
      "Valid tokens: [['tr', '\",', '\">', '\":', '\")', '\");', '\".', '\";', '\").', 'true', '\"]', '\"><', '\",\"', 'nu', 'null', 'false', '\"/>', '\":\"', '\"),', '\"></', 'fa', '\"))', '\"`', '\"?', '\"));', '\"}', '\">\\r', '\"];', '\"},', '\"],', '\")]', '\",\\r', '\"\"', '\"].', '\"+', 'fal', '\");\\r', '\"?>', '\"\\r', '\"=>', '\"])', '\")`', '\".$', '\"/', '\";\\r', '\"\\\\', '\":{\"', 't', 'n', 'f', '\"', '{', '['], [',', '}']]\n",
      "Argmax: tensor([ 4304, 29913], device='cuda:0')\n",
      "--------\n",
      "Decoded sequences: ['{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,null', '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,false']\n",
      "Parsing states: [(75, {'RSQB', 'COMMA'}), (75, {'RSQB', 'COMMA'})]\n",
      "Valid tokens: [[',', ']'], [',', ']']]\n",
      "Argmax: tensor([29892, 29892], device='cuda:0')\n",
      "--------\n",
      "{\"num_values\": \"4\", \"values\": [\"1\", \"2\",null,null,null,null,null,null,null,null,\n"
     ]
    }
   ],
   "source": [
    "num_beams = 2\n",
    "input_prompt = '{\"num_values\": \"4\", \"values\": [\"1\", \"2\",'\n",
    "max_length = 35\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    input_prompt, \n",
    "    return_tensors=\"pt\"\n",
    ").input_ids\n",
    "input_ids = torch.stack([input_ids] * num_beams, dim=0).reshape(num_beams, -1).to(model.device)\n",
    "bos_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long) * model.config.bos_token_id\n",
    "input_ids = torch.cat([bos_ids, input_ids], dim=-1)\n",
    "\n",
    "final_sentence = model.beam_search(\n",
    "    input_ids, \n",
    "    beam_scorer=BeamSearchScorer(\n",
    "        batch_size=1,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        device=\"cuda\",\n",
    "        length_penalty=1.0,\n",
    "        do_early_stopping=True,\n",
    "    ),\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        LogitsProcessor(tokenizer)\n",
    "    ]),\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        MaxLengthCriteria(max_length=max_length)\n",
    "    ]),\n",
    "    pad_token_id=tokenizer.eos_token_id, \n",
    ")\n",
    "\n",
    "final_sentence_str = tokenizer.batch_decode(final_sentence, skip_special_tokens=True)[0]\n",
    "print(final_sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
